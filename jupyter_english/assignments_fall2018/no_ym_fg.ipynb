{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, hstack, issparse\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "# from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Conv1D, MaxPooling1D, Softmax, Add, Flatten, Activation, LSTM, Bidirectional\n",
    "from keras.layers import Dropout, BatchNormalization, TimeDistributed, GRU, Reshape\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import to_categorical\n",
    "from keras.regularizers import l2, l1\n",
    "\n",
    "import functools\n",
    "\n",
    "import types\n",
    "import copy\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "import tensorflow as tf\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1,\n",
    "                              inter_op_parallelism_threads=1,\n",
    "#                               device_count = {'GPU' : 0}\n",
    "                             )\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "seed(17)\n",
    "set_random_seed(17)\n",
    "\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function for writing predictions to a file\n",
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)\n",
    "\n",
    "\n",
    "def add_time_features(df, X_sparse):\n",
    "    hour = df['time1'].apply(lambda ts: ts.hour)\n",
    "    morning = ((hour >= 8) & (hour <= 11)).astype('int')\n",
    "    day = ((hour >= 12) & (hour <= 18)).astype('int')\n",
    "    evening = ((hour >= 19) & (hour <= 23)).astype('int')\n",
    "    night = ((hour >= 0) & (hour <= 7)).astype('int')\n",
    "\n",
    "    weekday = df['time1'].apply(lambda ts: ts.weekday())\n",
    "#    weekend = (weekday <= 4).astype('int')\n",
    "\n",
    "\n",
    "    min_d = df[times].min(axis=1)\n",
    "    max_d = df[times].max(axis=1)\n",
    "\n",
    "    # Calculate sessions' duration in seconds\n",
    "    seconds = (max_d - min_d) / np.timedelta64(1, 's')\n",
    "\n",
    "    n_unique_sites = df[df[sites] != 0][sites].apply(\n",
    "            lambda site: site[site != 0].nunique(),\n",
    "            axis=1).astype('float64')\n",
    "\n",
    "    X = hstack([\n",
    "            X_sparse,\n",
    "            morning.values.reshape(-1, 1),\n",
    "            day.values.reshape(-1, 1),\n",
    "            evening.values.reshape(-1, 1),\n",
    "            night.values.reshape(-1, 1),\n",
    "            seconds.values.reshape(-1, 1),\n",
    "            n_unique_sites.values.reshape(-1, 1)\n",
    "#            weekday.values.reshape(-1, 1)\n",
    "#            weekend.values.reshape(-1, 1)\n",
    "            ])\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def as_keras_metric(method):\n",
    "#     @functools.wraps(method)\n",
    "#     def wrapper(self, args, **kwargs):\n",
    "#         \"\"\" Wrapper for turning tensorflow metrics into keras metrics \"\"\"\n",
    "#         value, update_op = method(self, args, **kwargs)\n",
    "#         K.get_session().run(tf.local_variables_initializer())\n",
    "#         with tf.control_dependencies([update_op]):\n",
    "#             value = tf.identity(value)\n",
    "#         return value\n",
    "#     return wrapper\n",
    "# auc_roc = as_keras_metric(tf.metrics.auc)\n",
    "\n",
    "def auc_roc(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_clf(estimator, param_grid, Xtrain, ytrain, cv, scoring=auc_roc):\n",
    "    \"\"\"\n",
    "    CV using GridSearchCV for given estimator.\n",
    "    \"\"\"    \n",
    "    grid_nn = GridSearchCV(estimator=estimator, scoring=scoring, param_grid=param_grid, cv=cv)\n",
    "    grid_result_nn = grid_nn.fit(Xtrain, ytrain)\n",
    "    \n",
    "    # summarize results\n",
    "    print(\"Best: %f using %s\" % (grid_result_nn.best_score_, grid_result_nn.best_params_))\n",
    "    means = grid_result_nn.cv_results_['mean_test_score']\n",
    "    stds = grid_result_nn.cv_results_['std_test_score']\n",
    "    params = grid_result_nn.cv_results_['params']\n",
    "    for mean, stdev, param in zip(means, stds, params):\n",
    "        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "        \n",
    "    return grid_result_nn\n",
    "    \n",
    "def grid_logit_model(\n",
    "    Xtrain, \n",
    "    ytrain, \n",
    "#     scoring=auc_roc,\n",
    "    cv=StratifiedKFold(n_splits=3),\n",
    "    param_grid=None):\n",
    "    \"\"\"\n",
    "    CV using GridSearchCV for LogisticRegression model.\n",
    "    \"\"\"    \n",
    "    clf = LogisticRegression(\n",
    "        multi_class='ovr', \n",
    "        solver='saga',\n",
    "        random_state=17, \n",
    "        n_jobs=-1)\n",
    "\n",
    "    if param_grid is None:\n",
    "        Cs = [1, 0.01]\n",
    "        param_grid = dict(\n",
    "            C=Cs,\n",
    "            multi_class=['ovr']\n",
    "        )\n",
    "    \n",
    "    return grid_clf(clf, param_grid, Xtrain, ytrain, cv, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasBatchClassifier(KerasClassifier):\n",
    "    \"\"\"\n",
    "    Add fit_generator to KerasClassifier to convert sparse matrices to numpy arrays before fitting.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        if not issparse(X):\n",
    "            return super().fit(X, y, **kwargs)\n",
    "\n",
    "        # taken from keras.wrappers.scikit_learn.KerasClassifier.fit ###################################################\n",
    "        if self.build_fn is None:\n",
    "            self.model = self.__call__(**self.filter_sk_params(self.__call__))\n",
    "        elif not isinstance(self.build_fn, types.FunctionType) and not isinstance(self.build_fn, types.MethodType):\n",
    "            self.model = self.build_fn(**self.filter_sk_params(self.build_fn.__call__))\n",
    "        else:\n",
    "            self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n",
    "\n",
    "        loss_name = self.model.loss\n",
    "        if hasattr(loss_name, '__name__'):\n",
    "            loss_name = loss_name.__name__\n",
    "        if loss_name == 'categorical_crossentropy' and len(y.shape) != 2:\n",
    "            y = to_categorical(y)\n",
    "\n",
    "        fit_args = copy.deepcopy(self.filter_sk_params(Sequential.fit_generator))\n",
    "        fit_args.update(kwargs)\n",
    "        ################################################################################################################\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor=\"val_loss\", \n",
    "                                       patience=3, \n",
    "#                                        verbose=1, \n",
    "                                       mode=\"auto\")\n",
    "        model_checkpoint = ModelCheckpoint(\"results/best_weights.{epoch:02d}.hdf5\", \n",
    "                                           monitor=\"val_loss\", \n",
    "#                                            verbose=1, \n",
    "                                           save_best_only=True,\n",
    "                                           mode=\"auto\")\n",
    "        \n",
    "        callbacks = [early_stopping, model_checkpoint]\n",
    "        fit_args.update({\"callbacks\": callbacks})\n",
    "\n",
    "        self.__history = self.model.fit_generator(\n",
    "            self.batch_generator(X, y, batch_size=self.sk_params[\"batch_size\"]),\n",
    "#             samples_per_epoch=X.shape[0],\n",
    "            steps_per_epoch=X.shape[0] // self.sk_params[\"batch_size\"],\n",
    "            **fit_args)\n",
    "\n",
    "        return self.__history\n",
    "\n",
    "#     def score(self, X, y, **kwargs):\n",
    "#         kwargs = self.filter_sk_params(Sequential.evaluate, kwargs)\n",
    "\n",
    "#         # sparse to numpy array\n",
    "#         X = KerasBatchClassifier.sparse_to_array(X)\n",
    "\n",
    "#         loss_name = self.model.loss\n",
    "#         if hasattr(loss_name, '__name__'):\n",
    "#             loss_name = loss_name.__name__\n",
    "#         if loss_name == 'categorical_crossentropy' and len(y.shape) != 2:\n",
    "#             y = to_categorical(y)\n",
    "#         outputs = self.model.evaluate(X, y, **kwargs)\n",
    "#         if type(outputs) is not list:\n",
    "#             outputs = [outputs]\n",
    "#         for name, output in zip(self.model.metrics_names, outputs):\n",
    "#             if name == 'acc':\n",
    "#                 return output\n",
    "#         raise Exception('The model is not configured to compute accuracy. '\n",
    "#                         'You should pass `metrics=[\"accuracy\"]` to '\n",
    "#                         'the `model.compile()` method.')\n",
    "\n",
    "    def predict(self, X, batch_size=128):\n",
    "        return self.predict_proba(self, X, batch_size=batch_size)[:, 1] >= 0.5\n",
    "#         pred_generator = KerasBatchClassifier.batch_generator\n",
    "#         add_part = 1 if X.shape[0] % batch_size != 0 else 0\n",
    "#         return self.model.predict_generator(\n",
    "#             red_generator(X, None, batch_size), X.shape[0] // batch_size + add_part)[:, 1] >= 0.5\n",
    "\n",
    "    def predict_proba(self, X, batch_size=128):\n",
    "        pred_generator = KerasBatchClassifier.batch_generator\n",
    "        add_part = 1 if X.shape[0] % batch_size != 0 else 0\n",
    "        return self.model.predict_generator(\n",
    "            pred_generator(X, None, batch_size=batch_size), X.shape[0] // batch_size + add_part)\n",
    "\n",
    "    @staticmethod\n",
    "    def batch_generator(x, y=None, batch_size=128, predict_mode = False):\n",
    "        \"\"\" batch generator to enable sparse input \"\"\"\n",
    "        index = np.arange(x.shape[0])\n",
    "        start = 0\n",
    "        hasData = True\n",
    "        while hasData:\n",
    "#             if start == 0 and y is not None:\n",
    "#                 np.random.shuffle(index)\n",
    "            batch = index[start:start + batch_size]\n",
    "            if y is not None:\n",
    "                yield KerasBatchClassifier.sparse_to_array(x[batch]), y[batch]\n",
    "            else:\n",
    "                yield KerasBatchClassifier.sparse_to_array(x[batch])\n",
    "                \n",
    "            hasData = (start + batch_size) < x.shape[0]\n",
    "            start += batch_size\n",
    "            if not predict_mode:\n",
    "                if start >= x.shape[0]:\n",
    "                    start = 0\n",
    "                    \n",
    "                hasData = True\n",
    "\n",
    "        if predict_mode:\n",
    "            remain_part = start % x.shape[0]\n",
    "            if (start - remain_part) < batch_size:\n",
    "                start = start - batch_size\n",
    "                remain_part = x.shape[0] - start\n",
    "                print('yield last! start={} remain_part={}'.format(start, remain_part))\n",
    "                batch = index[start:start + remain_part]\n",
    "                yield KerasBatchClassifier.sparse_to_array(x[batch])\n",
    "\n",
    "#         while True:\n",
    "#             for Xy in zip(x, y):\n",
    "#                 yield (KerasBatchClassifier.sparse_to_array(Xy[0]), Xy[1])\n",
    "\n",
    "#     def on_epoch_end(self):\n",
    "#         'Updates indexes after each epoch'\n",
    "#         self.start = 0\n",
    "#         if self.shuffle == True:\n",
    "#             np.random.shuffle(self.indexes)\n",
    "            \n",
    "    @staticmethod\n",
    "    def sparse_to_array(sparse_list):\n",
    "#         print('sparse_list.shape={}'.format(sparse_list.shape))\n",
    "        nn = np.expand_dims(sparse_list.toarray(), 2)\n",
    "#         print('nn.shape={}'.format(nn.shape))\n",
    "        return nn\n",
    "#         array_list = []\n",
    "#         for s in sparse_list:\n",
    "#             array_list.append(s.toarray().astype(np.int8))\n",
    "#         return np.expand_dims(np.array(array_list), 1)\n",
    "#         return np.array(array_list)\n",
    "\n",
    "    @property\n",
    "    def history(self):\n",
    "        return self.__history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DataGenerator(keras.utils.Sequence):\n",
    "#     'Generates data for Keras'\n",
    "#     def __init__(self, x, y=None, batch_size=32, shuffle=True, predict_mode = False):\n",
    "#         'Initialization'\n",
    "#         self.batch_size = batch_size\n",
    "#         self.y = y\n",
    "#         self.x = x\n",
    "#         self.predict_mode = predict_mode\n",
    "#         self.shuffle = shuffle\n",
    "#         self.on_epoch_end()\n",
    "\n",
    "#     def __len__(self):\n",
    "#         'Denotes the number of batches per epoch'\n",
    "#         return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         'Generate one batch of data'\n",
    "#         # Generate indexes of the batch\n",
    "#         indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "#         # Find list of IDs\n",
    "#         list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "#         # Generate data\n",
    "#         X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "#         return X, y\n",
    "\n",
    "#     def on_epoch_end(self):\n",
    "#         'Updates indexes after each epoch'\n",
    "#         self.indexes = np.arange(len(self.list_IDs))\n",
    "#         if self.shuffle == True:\n",
    "#             np.random.shuffle(self.indexes)\n",
    "\n",
    "#     def __data_generation(self, list_IDs_temp):\n",
    "#         'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "#         # Initialization\n",
    "#         X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "#         y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "#         # Generate data\n",
    "#         for i, ID in enumerate(list_IDs_temp):\n",
    "#             # Store sample\n",
    "#             X[i,] = np.load('data/' + ID + '.npy')\n",
    "\n",
    "#             # Store class\n",
    "#             y[i] = self.labels[ID]\n",
    "\n",
    "#         return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def focal_loss(gamma=2., alpha=.25):\n",
    "\n",
    "#     gamma = float(gamma)\n",
    "#     alpha = float(alpha)\n",
    "\n",
    "#     def focal_loss_fixed(y_true, y_pred):\n",
    "#         \"\"\"Focal loss for multi-classification\n",
    "#         FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)\n",
    "#         Notice: y_pred is probability after softmax\n",
    "#         gradient is d(Fl)/d(p_t) not d(Fl)/d(x) as described in paper\n",
    "#         d(Fl)/d(p_t) * [p_t(1-p_t)] = d(Fl)/d(x)\n",
    "#         Focal Loss for Dense Object Detection\n",
    "#         https://arxiv.org/abs/1708.02002\n",
    "\n",
    "#         Arguments:\n",
    "#             y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]\n",
    "#             y_pred {tensor} -- model's output, shape of [batch_size, num_cls]\n",
    "\n",
    "#         Keyword Arguments:\n",
    "#             gamma {float} -- (default: {2.0})\n",
    "#             alpha {float} -- (default: {4.0})\n",
    "\n",
    "#         Returns:\n",
    "#             [tensor] -- loss.\n",
    "#         \"\"\"\n",
    "#         epsilon = 1.e-5\n",
    "#         print(y_pred.shape)\n",
    "#         y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
    "#         y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
    "\n",
    "#         model_out = tf.add(y_pred, epsilon)\n",
    "#         ce = tf.multiply(y_true, -tf.log(model_out))\n",
    "#         weight = tf.multiply(y_true, tf.pow(tf.subtract(1., model_out), gamma))\n",
    "#         fl = tf.multiply(alpha, tf.multiply(weight, ce))\n",
    "#         reduced_fl = tf.reduce_max(fl, axis=1)\n",
    "#         return tf.reduce_mean(reduced_fl)\n",
    "#     return focal_loss_fixed\n",
    "\n",
    "# def focal_loss(gamma=2., alpha=.25):\n",
    "#     def focal_loss_fixed(y_true, y_pred):\n",
    "#         pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "#         pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "#         return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "\n",
    "#     return focal_loss_fixed\n",
    "\n",
    "# def focal_loss(gamma=2, alpha=0.25):\n",
    "#     def focal_loss_fixed(y_true, y_pred):#with tensorflow\n",
    "#         eps = 1e-4\n",
    "#         y_pred=K.clip(y_pred,eps,1.-eps)#improve the stability of the focal loss and see issues 1 for more information\n",
    "#         pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "#         pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "#         return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n",
    "#     return focal_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasModelCreator():\n",
    "    def __init__(\n",
    "        self, \n",
    "        feature,\n",
    "        depth,\n",
    "        metrics=['accuracy'],\n",
    "        filters=32, #32\n",
    "        pool_size=5, # 5\n",
    "        random_state=17):\n",
    "        \n",
    "        self.feature=feature\n",
    "        self.depth=depth\n",
    "        self.filters=filters\n",
    "        self.pool_size=pool_size\n",
    "        self.random_state=random_state\n",
    "        self.metrics=metrics\n",
    "    \n",
    "    def __call__(\n",
    "        self, \n",
    "        optimizer='adam', \n",
    "        init='glorot_uniform', \n",
    "        loss='categorical_crossentropy', #'sparse_categorical_crossentropy'\n",
    "#         metrics=[auc_roc],\n",
    "        **sk_params):\n",
    "        \n",
    "        seed(self.random_state)\n",
    "        set_random_seed(self.random_state)\n",
    "\n",
    "        filters=self.filters\n",
    "        pool_size=self.pool_size\n",
    "        kernel_size=2\n",
    "        \n",
    "        print('feature={} depth={} filters={}'.format(self.feature, self.depth, filters))\n",
    "        inp = Input(shape=(self.feature, self.depth))\n",
    "        \n",
    "        hidden_size = 2\n",
    "        dropout_rate = 0.1\n",
    "        regul_coef_dense = 0.01\n",
    "        \n",
    "        C = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, kernel_initializer=init,\n",
    "                   kernel_regularizer=l2(regul_coef_dense),\n",
    "                   input_shape=(self.feature, self.depth))(inp)\n",
    "        C11 = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, kernel_initializer=init, padding='same')(C)\n",
    "        A11 = Activation(\"sigmoid\")(C11)\n",
    "        C12 = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, kernel_initializer=init, padding='same')(A11)\n",
    "        S11 = Add()([C12, C])\n",
    "        A12 = Activation(\"sigmoid\")(S11)\n",
    "        M11 = MaxPooling1D(pool_size=pool_size, strides=2)(A12)\n",
    "        \n",
    "#         C21 = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, kernel_initializer=init, padding='same')(M11)\n",
    "#         A21 = Activation(\"relu\")(C21)\n",
    "#         C22 = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, kernel_initializer=init, padding='same')(A21)\n",
    "#         S21 = Add()([C22, M11])\n",
    "#         A22 = Activation(\"relu\")(S21)\n",
    "#         M21 = MaxPooling1D(pool_size=pool_size, strides=2)(A22)\n",
    "        \n",
    "#         C31 = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, kernel_initializer=init, padding='same')(M21)\n",
    "#         A31 = Activation(\"relu\")(C31)\n",
    "#         C32 = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, kernel_initializer=init, padding='same')(A31)\n",
    "#         S31 = Add()([C32, M21])\n",
    "#         A32 = Activation(\"relu\")(S31)\n",
    "#         M31 = MaxPooling1D(pool_size=pool_size, strides=2)(A32)\n",
    "\n",
    "        output = Dropout(rate=dropout_rate)(M11)\n",
    "        \n",
    "#         output = Bidirectional(\n",
    "#         output = \\\n",
    "#              LSTM(hidden_size, \n",
    "#                    activation='tanh',\n",
    "#                    dropout=dropout_rate,\n",
    "#                    return_sequences=False,\n",
    "#              )(output)#(inp)\n",
    "    \n",
    "#         output = GRU(8, activation=\"relu\")(output)\n",
    "    \n",
    "#         output = Dropout(rate=dropout_rate)(output)\n",
    "#         output = Dense(pool_size, \n",
    "#                        activation=None,\n",
    "#                        kernel_regularizer=l2(regul_coef_dense)\n",
    "#                       )(output)\n",
    "        output = BatchNormalization()(output)\n",
    "        output = Activation('sigmoid')(output)\n",
    "        output = Dropout(rate=dropout_rate)(output)\n",
    "#         output = Dense(pool_size, \n",
    "#                        activation=None,\n",
    "#                        kernel_regularizer=l2(regul_coef_dense))(output)\n",
    "#         output = BatchNormalization()(output)\n",
    "#         output = TimeDistributed(Dense(pool_size))(output)\n",
    "\n",
    "\n",
    "#         F1 = output # Flatten()(output)\n",
    "        F1 = Flatten()(output)\n",
    "\n",
    "        D1 = Dense(filters, kernel_initializer=init)(F1)\n",
    "        A6 = Activation(\"sigmoid\")(D1)\n",
    "        D2 = Dense(filters, kernel_initializer=init)(A6)\n",
    "        D3 = Dense(pool_size, kernel_initializer=init)(D2)\n",
    "#         D3 = Dense(kernel_initializer=init)(D2)\n",
    "        A7 = Activation('sigmoid')(D3) # Softmax()(D3)\n",
    "        act_output = A7 # Reshape((2, -1))(A7)\n",
    "        model = Model(inputs=inp, outputs=act_output)\n",
    "        \n",
    "# #         C = inp  # Conv1D(filters=filters, kernel_size=kernel_size, strides=1, kernel_initializer=init)(inp)\n",
    "#         C = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, kernel_initializer=init,\n",
    "#                    kernel_regularizer=regularizers.l2(0.1),\n",
    "#                    input_shape=(self.feature, self.depth))(inp)\n",
    "\n",
    "#         C11 = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, kernel_initializer=init, padding='same')(C)\n",
    "#         A11 = Activation(\"relu\")(C11)\n",
    "#         C12 = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, kernel_initializer=init, padding='same')(A11)\n",
    "#         S11 = Add()([C12, C])\n",
    "#         A12 = Activation(\"relu\")(S11)\n",
    "#         M11 = MaxPooling1D(pool_size=pool_size, strides=2)(A12)\n",
    "\n",
    "\n",
    "#         C21 = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, kernel_initializer=init, padding='same')(M11)\n",
    "#         A21 = Activation(\"relu\")(C21)\n",
    "#         C22 = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, kernel_initializer=init, padding='same')(A21)\n",
    "#         S21 = Add()([C22, M11])\n",
    "#         A22 = Activation(\"relu\")(S21)\n",
    "#         M21 = MaxPooling1D(pool_size=pool_size, strides=2)(A22)\n",
    "\n",
    "\n",
    "#         C31 = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, kernel_initializer=init, padding='same')(M21)\n",
    "#         A31 = Activation(\"relu\")(C31)\n",
    "#         C32 = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, kernel_initializer=init, padding='same')(A31)\n",
    "#         S31 = Add()([C32, M21])\n",
    "#         A32 = Activation(\"relu\")(S31)\n",
    "#         M31 = MaxPooling1D(pool_size=pool_size, strides=2)(A32)\n",
    "\n",
    "\n",
    "#         C41 = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, kernel_initializer=init, padding='same')(M31)\n",
    "#         A41 = Activation(\"relu\")(C41)\n",
    "#         C42 = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, kernel_initializer=init, padding='same')(A41)\n",
    "#         S41 = Add()([C42, M31])\n",
    "#         A42 = Activation(\"relu\")(S41)\n",
    "#         M41 = MaxPooling1D(pool_size=pool_size, strides=2)(A42)\n",
    "\n",
    "\n",
    "#         C51 = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, kernel_initializer=init, padding='same')(M41)\n",
    "#         A51 = Activation(\"relu\")(C51)\n",
    "#         C52 = Conv1D(filters=filters, kernel_size=kernel_size, strides=1, kernel_initializer=init, padding='same')(A51)\n",
    "#         S51 = Add()([C52, M41])\n",
    "#         A52 = Activation(\"relu\")(S51)\n",
    "#         M51 = MaxPooling1D(pool_size=pool_size, strides=2)(A52)\n",
    "\n",
    "#         F1 = Flatten()(M21)#Flatten()(C)\n",
    "# #         F1 = Flatten()(M51)\n",
    "\n",
    "#         D1 = Dense(filters, kernel_initializer=init)(F1)\n",
    "#         A6 = Activation(\"relu\")(D1)\n",
    "#         D2 = Dense(filters, kernel_initializer=init)(A6)\n",
    "#         D3 = Dense(pool_size, kernel_initializer=init)(D2)\n",
    "#         A7 = Softmax()(D3)\n",
    "\n",
    "#         model = Model(inputs=inp, outputs=A7)\n",
    "\n",
    "\n",
    "        model.compile(loss=loss, optimizer=optimizer, \n",
    "#                       metrics =[auc_roc],\n",
    "                      metrics=self.metrics,\n",
    "                      **sk_params)\n",
    "        model.summary()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_clf_results(\n",
    "    clf, \n",
    "    Xtest, \n",
    "    ytest, \n",
    "    ytrain,\n",
    "    additional_title='',\n",
    "    show_metrics=True,\n",
    "    **kwargs):\n",
    "    \n",
    "    ypred = clf.predict_proba(Xtest, **kwargs)\n",
    "    \n",
    "    if show_metrics:\n",
    "        print('roc_auc: {}'.format(roc_auc_score(y_score=ypred, y_true=ytest)))\n",
    "        print('accuracy: {}'.format(accuracy_score(y_pred=ypred>=0.5, y_true=ytest)))\n",
    "        print('precision: {}'.format(precision_score(y_pred=ypred>=0.5, y_true=ytest, average='macro')))    \n",
    "        \n",
    "    ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_batch_generator(X_data, y_data, batch_size=100):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(y_data)[0])\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].todense()\n",
    "        y_batch = y_data[index_batch]\n",
    "        counter += 1\n",
    "        yield np.array(X_batch),y_batch\n",
    "        if (counter > number_of_batches):\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_model(\n",
    "    Xtrain, \n",
    "    ytrain, \n",
    "    Xtest, \n",
    "    ytest,\n",
    "    metrics=[auc_roc],\n",
    "    batch_size=100,\n",
    "    epochs=15,\n",
    "    filters=32,\n",
    "    pool_size=5,\n",
    "    additional_title='[NN]',\n",
    "    show_metrics=True,\n",
    "    verbose=1\n",
    "    ):\n",
    "    \n",
    "    seed(17)\n",
    "    set_random_seed(17)\n",
    "    \n",
    "    n_obs, feature = Xtrain.shape\n",
    "    depth = 1\n",
    "    \n",
    "    model_creator = KerasModelCreator(\n",
    "        metrics=metrics,\n",
    "        feature=feature, \n",
    "        depth=depth,\n",
    "        filters=filters,\n",
    "        pool_size=pool_size,\n",
    "        random_state=17)\n",
    "    \n",
    "    model_nn1 = KerasBatchClassifier(\n",
    "        build_fn=model_creator, \n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size, \n",
    "        verbose=verbose)\n",
    "    \n",
    "    \n",
    "    history = model_nn1.fit(\n",
    "#         nn_batch_generator(Xtrain, ytrain, batch_size=batch_size),\n",
    "#         samples_per_epoch=Xtrain.shape[0],\n",
    "        Xtrain,\n",
    "        ytrain, \n",
    "        epochs=epochs,\n",
    "#         batch_size=batch_size, \n",
    "        verbose=verbose, \n",
    "    #                     validation_data=(X_test, y_test_nn), \n",
    "    #                     callbacks=[lrate]\n",
    "                           )\n",
    "    return model_nn1, history\n",
    "#     return show_clf_results(\n",
    "#             model_nn1, \n",
    "#             Xtest, \n",
    "#             ytest, \n",
    "#             ytrain, \n",
    "#             additional_title,\n",
    "#             show_metrics=show_metrics\n",
    "#         ), model_nn1, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_nn_model(   \n",
    "    Xtrain, \n",
    "    ytrain, \n",
    "    scoring=auc_roc,\n",
    "    batch_size=100,\n",
    "    epochs=15,\n",
    "    filters=32,\n",
    "    pool_size=5,\n",
    "    cv=StratifiedKFold(n_splits=3),\n",
    "    param_grid=None):\n",
    "    \"\"\"\n",
    "    CV using GridSearchCV for NN model.\n",
    "    \"\"\" \n",
    "    n_obs, feature, depth = Xtrain.shape\n",
    "    model_creator = KerasModelCreator(\n",
    "        feature=feature, \n",
    "        depth=depth,\n",
    "        filters=filters,\n",
    "        pool_size=pool_size,\n",
    "        random_state=17)\n",
    "\n",
    "    clf = KerasBatchClassifier(build_fn=model_creator, \n",
    "                          epochs=2, \n",
    "                          batch_size=batch_size, \n",
    "                          verbose=1)\n",
    "    if param_grid is None:\n",
    "        optimizers = ['rmsprop', 'adam']\n",
    "        init = ['glorot_uniform', 'normal', 'uniform']\n",
    "        metrics=[[auc_roc]]#'['categorical_accuracy', 'accuracy']\n",
    "        epochs = [1, 2]\n",
    "        batches = [50, 100]\n",
    "        param_grid = dict(\n",
    "            optimizer=optimizers, \n",
    "            epochs=epochs, \n",
    "            batch_size=batches, \n",
    "            init=init,\n",
    "#             metrics=metrics\n",
    "        )\n",
    "    \n",
    "    return grid_clf(clf, param_grid, Xtrain, ytrain, cv, scoring=scoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21669</th>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54843</th>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2013-01-12 09:07:07</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 09:07:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77292</th>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:13</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:14</td>\n",
       "      <td>951.0</td>\n",
       "      <td>2013-01-12 08:50:15</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:15</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>784.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>949.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114021</th>\n",
       "      <td>945</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>949.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>945.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>945.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146670</th>\n",
       "      <td>947</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>950.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>950.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>951.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1               time1  site2               time2  site3  \\\n",
       "session_id                                                                \n",
       "21669          56 2013-01-12 08:05:57   55.0 2013-01-12 08:05:57    NaN   \n",
       "54843          56 2013-01-12 08:37:23   55.0 2013-01-12 08:37:23   56.0   \n",
       "77292         946 2013-01-12 08:50:13  946.0 2013-01-12 08:50:14  951.0   \n",
       "114021        945 2013-01-12 08:50:17  948.0 2013-01-12 08:50:17  949.0   \n",
       "146670        947 2013-01-12 08:50:20  950.0 2013-01-12 08:50:20  948.0   \n",
       "\n",
       "                         time3  site4               time4  site5  \\\n",
       "session_id                                                         \n",
       "21669                      NaT    NaN                 NaT    NaN   \n",
       "54843      2013-01-12 09:07:07   55.0 2013-01-12 09:07:09    NaN   \n",
       "77292      2013-01-12 08:50:15  946.0 2013-01-12 08:50:15  946.0   \n",
       "114021     2013-01-12 08:50:18  948.0 2013-01-12 08:50:18  945.0   \n",
       "146670     2013-01-12 08:50:20  947.0 2013-01-12 08:50:21  950.0   \n",
       "\n",
       "                         time5  ...                 time6  site7  \\\n",
       "session_id                      ...                                \n",
       "21669                      NaT  ...                   NaT    NaN   \n",
       "54843                      NaT  ...                   NaT    NaN   \n",
       "77292      2013-01-12 08:50:16  ...   2013-01-12 08:50:16  948.0   \n",
       "114021     2013-01-12 08:50:18  ...   2013-01-12 08:50:18  947.0   \n",
       "146670     2013-01-12 08:50:21  ...   2013-01-12 08:50:21  946.0   \n",
       "\n",
       "                         time7  site8               time8  site9  \\\n",
       "session_id                                                         \n",
       "21669                      NaT    NaN                 NaT    NaN   \n",
       "54843                      NaT    NaN                 NaT    NaN   \n",
       "77292      2013-01-12 08:50:16  784.0 2013-01-12 08:50:16  949.0   \n",
       "114021     2013-01-12 08:50:19  945.0 2013-01-12 08:50:19  946.0   \n",
       "146670     2013-01-12 08:50:21  951.0 2013-01-12 08:50:22  946.0   \n",
       "\n",
       "                         time9 site10              time10 target  \n",
       "session_id                                                        \n",
       "21669                      NaT    NaN                 NaT      0  \n",
       "54843                      NaT    NaN                 NaT      0  \n",
       "77292      2013-01-12 08:50:17  946.0 2013-01-12 08:50:17      0  \n",
       "114021     2013-01-12 08:50:19  946.0 2013-01-12 08:50:20      0  \n",
       "146670     2013-01-12 08:50:22  947.0 2013-01-12 08:50:22      0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../../data/websites_train_sessions.csv',\n",
    "                       index_col='session_id')\n",
    "test_df = pd.read_csv('../../data/websites_test_sessions.csv',\n",
    "                      index_col='session_id')\n",
    "\n",
    "# Convert time1, ..., time10 columns to datetime type\n",
    "times = ['time%s' % i for i in range(1, 11)]\n",
    "sites = ['site%s' % i for i in range(1, 11)]\n",
    "train_df[times] = train_df[times].apply(pd.to_datetime)\n",
    "test_df[times] = test_df[times].apply(pd.to_datetime)\n",
    "\n",
    "# Sort the data by time\n",
    "train_df = train_df.sort_values(by='time1')\n",
    "\n",
    "# Look at the first rows of the training set\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21669</th>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54843</th>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2013-01-12 09:07:07</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 09:07:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77292</th>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:13</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:14</td>\n",
       "      <td>951.0</td>\n",
       "      <td>2013-01-12 08:50:15</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:15</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>784.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>949.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114021</th>\n",
       "      <td>945</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>949.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>945.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>945.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146670</th>\n",
       "      <td>947</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>950.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>950.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>951.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1               time1  site2               time2  site3  \\\n",
       "session_id                                                                \n",
       "21669          56 2013-01-12 08:05:57   55.0 2013-01-12 08:05:57    NaN   \n",
       "54843          56 2013-01-12 08:37:23   55.0 2013-01-12 08:37:23   56.0   \n",
       "77292         946 2013-01-12 08:50:13  946.0 2013-01-12 08:50:14  951.0   \n",
       "114021        945 2013-01-12 08:50:17  948.0 2013-01-12 08:50:17  949.0   \n",
       "146670        947 2013-01-12 08:50:20  950.0 2013-01-12 08:50:20  948.0   \n",
       "\n",
       "                         time3  site4               time4  site5  \\\n",
       "session_id                                                         \n",
       "21669                      NaT    NaN                 NaT    NaN   \n",
       "54843      2013-01-12 09:07:07   55.0 2013-01-12 09:07:09    NaN   \n",
       "77292      2013-01-12 08:50:15  946.0 2013-01-12 08:50:15  946.0   \n",
       "114021     2013-01-12 08:50:18  948.0 2013-01-12 08:50:18  945.0   \n",
       "146670     2013-01-12 08:50:20  947.0 2013-01-12 08:50:21  950.0   \n",
       "\n",
       "                         time5  ...                 time6  site7  \\\n",
       "session_id                      ...                                \n",
       "21669                      NaT  ...                   NaT    NaN   \n",
       "54843                      NaT  ...                   NaT    NaN   \n",
       "77292      2013-01-12 08:50:16  ...   2013-01-12 08:50:16  948.0   \n",
       "114021     2013-01-12 08:50:18  ...   2013-01-12 08:50:18  947.0   \n",
       "146670     2013-01-12 08:50:21  ...   2013-01-12 08:50:21  946.0   \n",
       "\n",
       "                         time7  site8               time8  site9  \\\n",
       "session_id                                                         \n",
       "21669                      NaT    NaN                 NaT    NaN   \n",
       "54843                      NaT    NaN                 NaT    NaN   \n",
       "77292      2013-01-12 08:50:16  784.0 2013-01-12 08:50:16  949.0   \n",
       "114021     2013-01-12 08:50:19  945.0 2013-01-12 08:50:19  946.0   \n",
       "146670     2013-01-12 08:50:21  951.0 2013-01-12 08:50:22  946.0   \n",
       "\n",
       "                         time9 site10              time10 target  \n",
       "session_id                                                        \n",
       "21669                      NaT    NaN                 NaT      0  \n",
       "54843                      NaT    NaN                 NaT      0  \n",
       "77292      2013-01-12 08:50:17  946.0 2013-01-12 08:50:17      0  \n",
       "114021     2013-01-12 08:50:19  946.0 2013-01-12 08:50:20      0  \n",
       "146670     2013-01-12 08:50:22  947.0 2013-01-12 08:50:22      0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert time1, ..., time10 columns to datetime type\n",
    "times = ['time%s' % i for i in range(1, 11)]\n",
    "sites = ['site%s' % i for i in range(1, 11)]\n",
    "train_df[times] = train_df[times].apply(pd.to_datetime)\n",
    "test_df[times] = test_df[times].apply(pd.to_datetime)\n",
    "\n",
    "# Sort the data by time\n",
    "train_df = train_df.sort_values(by='time1')\n",
    "\n",
    "# Look at the first rows of the training set\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/site_dic.pkl', \"rb\") as inp_file:\n",
    "    site_dic = pickle.load(inp_file)\n",
    "\n",
    "inv_site_dic = {v: k for k, v in site_dic.items()}\n",
    "# inv_site_dic.update({0: ''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[sites] = train_df[sites].fillna(0)\n",
    "test_df[sites] = test_df[sites].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AAA():\n",
    "    X_train = train_df[sites].apply(\n",
    "            lambda x: \" \".join(\n",
    "                    [inv_site_dic[a] for a in x.values if a != 0]), axis=1)\n",
    "\n",
    "    X_train = X_train.apply(lambda x: x.replace('.', ' '))\n",
    "\n",
    "    X_test = test_df[sites].apply(\n",
    "            lambda x: \" \".join(\n",
    "                    [inv_site_dic[a] for a in x.values if a != 0]), axis=1)\n",
    "\n",
    "    X_test = X_test.apply(lambda x: x.replace('.', ' '))\n",
    "\n",
    "    y_train = train_df['target'].astype('int')\n",
    "\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "            [(\"vectorize\",\n",
    "              TfidfVectorizer(ngram_range=(1, 3),\n",
    "                              max_features=100000)),  # 100000!!!!\n",
    "            (\"tfidf\", TfidfTransformer())])\n",
    "\n",
    "    pipeline.fit(X_train.ravel(), y_train)\n",
    "\n",
    "\n",
    "    X_train = pipeline.transform(X_train.ravel())\n",
    "    X_test = pipeline.transform(X_test.ravel())\n",
    "\n",
    "\n",
    "    print(type(X_train))    # scipy.sparse.csr.csr_matrix\n",
    "\n",
    "    print(X_train.shape)   # (253561, 250000)\n",
    "\n",
    "\n",
    "    X_train = add_time_features(train_df, X_train)\n",
    "    X_test = add_time_features(test_df, X_test)\n",
    "\n",
    "\n",
    "    print(X_train.shape, X_test.shape)  #  ((253561, 250004), (82797, 250004))\n",
    "\n",
    "    time_split = TimeSeriesSplit(n_splits=12)\n",
    "    logit = LogisticRegression(C=1, random_state=17)\n",
    "\n",
    "    # c_values = np.logspace(-2,2, 10)\n",
    "    # c_values = np.arange(0,5.,step=0.5)\n",
    "    # c_values = np.concatenate((np.arange(0.1,1,0.1), np.arange(1,5,0.5)))\n",
    "    c_values = np.concatenate((np.arange(0.5, 2,step=0.5), np.arange(2, 3.6, 0.1)))\n",
    "\n",
    "    logit_grid_searcher = GridSearchCV(\n",
    "            estimator=logit,\n",
    "            param_grid={'C': c_values},\n",
    "            scoring='auc',\n",
    "            n_jobs=-1,\n",
    "            cv=time_split,\n",
    "            verbose=10)\n",
    "\n",
    "\n",
    "    logit_grid_searcher.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    print(logit_grid_searcher.best_score_, logit_grid_searcher.best_params_)\n",
    "    # (0.8884620990228279, {'C': 3.5000000000000013})\n",
    "    # weekend+weekday 0.876261489918 {'C': 3.5000000000000013}\n",
    "    # weekday  0.876293321626 {'C': 3.5000000000000013}\n",
    "    # def : 100000   0.890742942071 {'C': 3.5000000000000013}\n",
    "    #                0.890858325471 {'C': 4.200000000000002}\n",
    "    #      +seconds+n_unique_sites  0.893673781638 {'C': 3.4000000000000012}\n",
    "\n",
    "    logit_test_pred = logit_grid_searcher.predict_proba(X_test)[:, 1]\n",
    "    write_to_submission_file(logit_test_pred, 'submit.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special transformer to save output shape\n",
    "class ShapeSaver(BaseEstimator, TransformerMixin):\n",
    "    def transform(self, X):\n",
    "        self.shape = X.shape\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "## Helper functions that extract different data\n",
    "#####################################\n",
    "\n",
    "# Return sites columns as a single string\n",
    "# This string can be supplied into CountVectorizer or TfidfVectorizer\n",
    "\n",
    "def extract_sites_as_string(X):\n",
    "    #return X[sites].astype('str').apply(' '.join, axis=1)\n",
    "    return X['sites_str']\n",
    "\n",
    "\n",
    "\n",
    "# Year-month feature from A4\n",
    "def feature_year_month(X):\n",
    "    return pd.DataFrame(X['time1'].dt.year * 100 + X['time1'].dt.month)\n",
    "\n",
    "def feature_year_month_log1p(X):\n",
    "    return pd.DataFrame(np.log1p(X['time1'].dt.year * 100 + X['time1'].dt.month))\n",
    "\n",
    "# yearfeature from A4\n",
    "def feature_year(X):\n",
    "    return pd.DataFrame(X['time1'].dt.year)\n",
    "\n",
    "# Hour feature from A4\n",
    "def feature_hour(X):\n",
    "    return pd.DataFrame(X['time1'].dt.hour)\n",
    "\n",
    "# Hour feature from A4\n",
    "def feature_hour_log(X):\n",
    "    return np.log1p(pd.DataFrame(X['time1'].dt.hour))\n",
    "\n",
    "\n",
    "# Month\n",
    "def feature_month(X):\n",
    "    return pd.DataFrame(X['time1'].dt.month)\n",
    "\n",
    "# Weekday\n",
    "def feature_weekday(X):\n",
    "    return pd.DataFrame(X['time1'].dt.weekday)\n",
    "\n",
    "# Is day feature from A4\n",
    "def feature_is_daytime(X):\n",
    "    return pd.DataFrame( (X['time1'].dt.hour >= 12) & (X['time1'].dt.hour <= 18))\n",
    "\n",
    "# Is evening feature from A4\n",
    "def feature_is_evening(X):\n",
    "    return pd.DataFrame( (X['time1'].dt.hour >= 19) & (X['time1'].dt.hour <= 23))\n",
    "\n",
    "# Is morning feature from A4\n",
    "def feature_is_morning(X):\n",
    "    return pd.DataFrame(X['time1'].dt.hour <= 11)\n",
    "\n",
    "# Long Session length feature from A4\n",
    "def feature_is_long_session(X):\n",
    "    X['session_end_time'] = X[times].max(axis=1)\n",
    "    session_duration = (X['session_end_time'] - X['time1']).astype('timedelta64[s]')\n",
    "#    q = session_duration.quantile([0.1, 0.90]).values\n",
    "    X['long_session_duration'] = 0\n",
    "    X[session_duration < 10]['long_session_duration'] = 1\n",
    "    X[session_duration < 20]['long_session_duration'] = 2\n",
    "    X[session_duration < 100]['long_session_duration'] = 3\n",
    "    X[session_duration < 500]['long_session_duration'] = 4\n",
    "    X[session_duration < 1000]['long_session_duration'] = 5\n",
    "#    X[(session_duration > q[1]) & (session_duration <= q[2])]['long_session_duration'] = 2\n",
    "#    X[(session_duration > q[2]) & (session_duration <= q[3])]['long_session_duration'] = 3\n",
    "#    X[(session_duration > q[3]) & (session_duration <= q[4])]['long_session_duration'] = 4\n",
    "#    X[session_duration > q[1]]['long_session_duration'] = 2\n",
    "    return X[['long_session_duration']]\n",
    "\n",
    "# Session length feature from A4\n",
    "def feature_session_len(X):\n",
    "    X['session_end_time'] = X[times].max(axis=1)\n",
    "    X['session_duration'] = (X['session_end_time'] - X['time1']).astype('timedelta64[s]')\n",
    "    return X[['session_duration']]\n",
    "\n",
    "# uniq sites per session\n",
    "def feature_uniq_sites(X):\n",
    "    X['n_unique_sites'] = X[X[sites] != 0][sites].apply(\n",
    "            lambda site: site[site != 0].nunique(), axis=1).astype('float64')\n",
    "\n",
    "    return X[['n_unique_sites']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        # List of features goes here:\n",
    "#        ('year_month_val', Pipeline([\n",
    "#            ('extract', FunctionTransformer(feature_year_month, validate=False)),\n",
    "#            ('scale', StandardScaler()),\n",
    "#            ('shape', ShapeSaver())\n",
    "#        ])),\n",
    "        ('session_len', Pipeline([\n",
    "            ('extract', FunctionTransformer(feature_session_len, validate=False)),\n",
    "            ('scale', StandardScaler()),\n",
    "            ('shape', ShapeSaver())\n",
    "        ])),\n",
    "        ('weekday_cat', Pipeline([\n",
    "            ('extract', FunctionTransformer(feature_weekday, validate=False)),\n",
    "            ('ohe', OneHotEncoder()),\n",
    "            ('shape', ShapeSaver())\n",
    "        ])),\n",
    "#        ('hour_val', Pipeline([\n",
    "#            ('extract', FunctionTransformer(feature_hour, validate=False)),\n",
    "##            ('scale', StandardScaler()),\n",
    "#            ('ohe', OneHotEncoder()),\n",
    "#            ('shape', ShapeSaver())\n",
    "#         ])),\n",
    "        ('hour_val_log1p', Pipeline([\n",
    "            ('extract', FunctionTransformer(feature_hour_log, validate=False)),\n",
    "            ('scale', StandardScaler()),\n",
    "            ('shape', ShapeSaver())\n",
    "         ])),\n",
    "        ('hour_cat', Pipeline([\n",
    "            ('extract', FunctionTransformer(feature_hour, validate=False)),\n",
    "            ('ohe', OneHotEncoder()),\n",
    "            ('shape', ShapeSaver())\n",
    "         ])),\n",
    "        ('month_cat', Pipeline([\n",
    "            ('extract', FunctionTransformer(feature_month, validate=False)),\n",
    "            ('ohe', OneHotEncoder()),\n",
    "            ('shape', ShapeSaver())\n",
    "         ])),\n",
    "        ('is_morning', Pipeline([\n",
    "            ('extract', FunctionTransformer(feature_is_morning, validate=False)),\n",
    "            ('shape', ShapeSaver())\n",
    "         ])),\n",
    "        ('is_daytime', Pipeline([\n",
    "            ('extract', FunctionTransformer(feature_is_daytime, validate=False)),\n",
    "            ('shape', ShapeSaver())\n",
    "         ])),\n",
    "        ('is_evening', Pipeline([\n",
    "            ('extract', FunctionTransformer(feature_is_evening, validate=False)),\n",
    "            ('shape', ShapeSaver())\n",
    "         ])),\n",
    "        ('is_long_session', Pipeline([\n",
    "            ('extract', FunctionTransformer(feature_is_long_session, validate=False)),\n",
    "            ('ohe', OneHotEncoder()),\n",
    "            ('shape', ShapeSaver())\n",
    "         ])),\n",
    "#        ('feature_uniq_sites', Pipeline([\n",
    "#            ('extract', FunctionTransformer(feature_uniq_sites, validate=False)),\n",
    "#            ('ohe', OneHotEncoder()),\n",
    "#            ('shape', ShapeSaver())\n",
    "#         ])),\n",
    "        ('year', Pipeline([\n",
    "            ('extract', FunctionTransformer(feature_year, validate=False)),\n",
    "            ('ohe', OneHotEncoder()),\n",
    "            ('shape', ShapeSaver())\n",
    "         ])),\n",
    "        ('sites_tfidf', Pipeline([\n",
    "            ('extract', FunctionTransformer(extract_sites_as_string, validate=False)),\n",
    "            ('count', TfidfVectorizer(token_pattern=r'(?u)\\b\\w+\\b',\n",
    "                                      ngram_range=(1, 3),\n",
    "                                      max_features=50000)),  # 100000!\n",
    "            (\"tfidf\", TfidfTransformer()),\n",
    "            ('shape', ShapeSaver())\n",
    "        ])),\n",
    "        # Add more features here :)\n",
    "        # ...\n",
    "    ]))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253561, 50044) (82797, 50044)\n"
     ]
    }
   ],
   "source": [
    "# Run preprocessing on full data\n",
    "x_train_new = train_df.iloc[:, :-1]\n",
    "x_train_new['sites_str'] = train_df[sites].apply(\n",
    "        lambda x: \" \".join(\n",
    "                [inv_site_dic[a] for a in x.values if a != 0]), axis=1)\n",
    "\n",
    "x_train_new['sites_str'] = x_train_new['sites_str'].apply(lambda x: x.replace('.', ' '))\n",
    "\n",
    "x_test_new = test_df.iloc[:, :]\n",
    "x_test_new['sites_str'] = test_df[sites].apply(\n",
    "        lambda x: \" \".join(\n",
    "                [inv_site_dic[a] for a in x.values if a != 0]), axis=1)\n",
    "\n",
    "x_test_new['sites_str'] = x_test_new['sites_str'].apply(lambda x: x.replace('.', ' '))\n",
    "\n",
    "transformed_train_df = transform_pipeline.fit_transform(x_train_new)\n",
    "transformed_test_df = transform_pipeline.transform(x_test_new)\n",
    "\n",
    "X_train_new = transformed_train_df\n",
    "y_train_new = train_df['target']\n",
    "X_test_new = transformed_test_df\n",
    "\n",
    "print(transformed_train_df.shape, transformed_test_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit_generator(generator=batch_generator(X_train_sparse, Y_train, batch_size),\n",
    "#                     nb_epoch=nb_epoch, \n",
    "#                     samples_per_epoch=X_train_sparse.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 100\n",
    "FILTERS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((253561, 50044), scipy.sparse.csr.csr_matrix)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_train_df.shape, type(transformed_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_train_data = 10000\n",
    "X_train_nn1 = X_train_new#[:max_train_data]# np.expand_dims(transformed_train_df, 2)\n",
    "X_test_nn1 = X_test_new# np.expand_dims(transformed_test_df, 2)\n",
    "y_train_nn1 = y_train_new#[:max_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((253561, 50044), (253561,), (82797, 50044), scipy.sparse.csr.csr_matrix)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_nn1.shape, y_train_nn1.shape, X_test_nn1.shape, type(X_test_nn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202848, 50044), (50713, 50044))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_part, X_valid, y_train_part, y_valid = \\\n",
    "        train_test_split(\n",
    "                X_train_nn1,\n",
    "                y_train_nn1,\n",
    "                test_size=0.20,\n",
    "                random_state=17,\n",
    "                stratify=y_train_nn1)\n",
    "\n",
    "X_train_part.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature=50044 depth=1 filters=20\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           (None, 50044, 1)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, 50043, 20)    60          input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, 50043, 20)    820         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 50043, 20)    0           conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, 50043, 20)    820         activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 50043, 20)    0           conv1d_42[0][0]                  \n",
      "                                                                 conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 50043, 20)    0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 25021, 20)    0           activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 25021, 20)    0           max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 25021, 20)    80          dropout_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 25021, 20)    0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_28 (Dropout)            (None, 25021, 20)    0           activation_59[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)            (None, 500420)       0           dropout_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 20)           10008420    flatten_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 20)           0           dense_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 20)           420         activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_42 (Dense)                (None, 2)            42          dense_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 2)            0           dense_42[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 10,010,662\n",
      "Trainable params: 10,010,622\n",
      "Non-trainable params: 40\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[20] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node batch_normalization_14/moments/mean}} = Mean[T=DT_FLOAT, Tidx=DT_INT32, _class=[\"loc:@train...ad/truediv\"], keep_dims=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dropout_27/cond/Merge, training_13/Adam/gradients/batch_normalization_14/moments/variance_grad/mod)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mD:\\DiskD\\Eldaniz\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-5a215ca17e29>\u001b[0m in \u001b[0;36mtrain_nn_model\u001b[1;34m(Xtrain, ytrain, Xtest, ytest, metrics, batch_size, epochs, filters, pool_size, additional_title, show_metrics, verbose)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;31m#         batch_size=batch_size,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[1;31m#                     validation_data=(X_test, y_test_nn),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;31m#                     callbacks=[lrate]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-36f980e2a70d>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;31m#             samples_per_epoch=X.shape[0],\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msk_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"batch_size\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             **fit_args)\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__history\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\DiskD\\Eldaniz\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\DiskD\\Eldaniz\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\DiskD\\Eldaniz\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\DiskD\\Eldaniz\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\DiskD\\Eldaniz\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\DiskD\\Eldaniz\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\DiskD\\Eldaniz\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\DiskD\\Eldaniz\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    529\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[20] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node batch_normalization_14/moments/mean}} = Mean[T=DT_FLOAT, Tidx=DT_INT32, _class=[\"loc:@train...ad/truediv\"], keep_dims=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dropout_27/cond/Merge, training_13/Adam/gradients/batch_normalization_14/moments/variance_grad/mod)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# with tf.device('/gpu:0'):\n",
    "    # y_pred_nn1, \n",
    "model_nn1, history = train_nn_model(\n",
    "    X_train_part,\n",
    "    y_train_part,\n",
    "    X_valid, \n",
    "    y_valid, \n",
    "    metrics=['accuracy'],\n",
    "    additional_title='[NN]',\n",
    "    pool_size=2,\n",
    "    show_metrics=True, \n",
    "    filters=FILTERS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=1)\n",
    "\n",
    "# with open('no_ym_fg_nn_model.pkl', 'wb') as f:\n",
    "#     pickle.dump(model_nn1, f)\n",
    "#     pickle.dump(history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function auc at 0x00000198C3556400>: it's not the same object as tensorflow.python.ops.metrics_impl.auc",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-5f80a3b1b5f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'no_ym_fg_nn_model.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_nn1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPicklingError\u001b[0m: Can't pickle <function auc at 0x00000198C3556400>: it's not the same object as tensorflow.python.ops.metrics_impl.auc"
     ]
    }
   ],
   "source": [
    "with open('no_ym_fg_nn_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model_nn1, f)\n",
    "    pickle.dump(history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_pred_nn__ = show_clf_results(\n",
    "#             model_nn1, \n",
    "#             X_valid, \n",
    "#             y_valid, \n",
    "#             y_train_part, \n",
    "#             additional_title='[NN]',\n",
    "#             show_metrics=True,\n",
    "#             batch_size=BATCH_SIZE\n",
    "#         )\n",
    "\n",
    "BATCH_SIZE, FILTERS, EPOCHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_nn__ = show_clf_results(\n",
    "#             model_nn1, \n",
    "#             X_valid, \n",
    "#             y_valid, \n",
    "#             y_train_part, \n",
    "#             additional_title='[NN]',\n",
    "#             show_metrics=True,\n",
    "#             batch_size=BATCH_SIZE\n",
    "#         )\n",
    "# y_pred = model_nn1.predict(X_valid)\n",
    "# X_valid.shape, y_pred.shape\n",
    "\n",
    "pred_generator = KerasBatchClassifier.batch_generator(X_valid, None, BATCH_SIZE, True)\n",
    "add_part = 1 if X_valid.shape[0] % BATCH_SIZE != 0 else 0\n",
    "y_pred_nn1 = model_nn1.model.predict_generator(pred_generator, X_valid.shape[0] // BATCH_SIZE + add_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50713, 50044), (50713, 2))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape, y_pred_nn1.shape  # 1000 => 127909, 500 ==> 64000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50713"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_pred_nn1[:, 1] < 0.1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc_auc: 0.5\n",
      "accuracy: 0.9909490663143573\n",
      "precision: 0.49547453315717865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\DiskD\\Eldaniz\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print('roc_auc: {}'.format(roc_auc_score(y_score=y_pred_nn1[:, 1], y_true=y_valid)))\n",
    "print('accuracy: {}'.format(accuracy_score(y_pred=y_pred_nn1[:, 1]>=0.5, y_true=y_valid)))\n",
    "print('precision: {}'.format(precision_score(y_pred=y_pred_nn1[:, 1]>=0.5, y_true=y_valid, average='macro')))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_ = model_nn1.predict_proba(X_valid, BATCH_SIZE)\n",
    "    \n",
    "\n",
    "print('roc_auc: {}'.format(roc_auc_score(y_score=ypred_, y_true=y_valid)))\n",
    "print('accuracy: {}'.format(accuracy_score(y_pred=ypred_, y_true=y_valid)))\n",
    "# print('precision: {}'.format(precision_score(y_pred=ypred_, y_true=y_valid, average='macro')))    \n",
    "\n",
    "ypred_\n",
    "# print('roc_auc: {}'.format(roc_auc_score(y_score=y_pred_nn1[:,0]>=0.1, y_true=y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 5 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   18.0s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   25.8s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   51.0s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  1.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  1.8min finished\n",
      "D:\\DiskD\\Eldaniz\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9931947218095609\n",
      "0.9104206626981047 {'C': 2.0309, 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "# time_split = TimeSeriesSplit(n_splits=12)\n",
    "\n",
    "# logit = LogisticRegression(C=1, random_state=17)\n",
    "\n",
    "# # c_values = np.logspace(-2,2, 10)\n",
    "# # c_values = np.arange(0,5.,step=0.5)\n",
    "# # c_values = np.concatenate((np.arange(0.1,1,0.1), np.arange(1,5,0.5)))\n",
    "# c_values = np.concatenate((np.arange(0.5, 2,step=0.5), np.arange(2, 3.6, 0.1)))\n",
    "\n",
    "\n",
    "# clf = LogisticRegression(C=1, random_state=17)  # RandomForestClassifier(random_state=17)\n",
    "\n",
    "# tree_params = {\n",
    "#         'max_depth': [2, 5, 10],\n",
    "#         'max_features': [3, 5, 20]}\n",
    "\n",
    "# #c_values = np.logspace(-4, 10, 40)\n",
    "# logit_params = {\n",
    "#         'C': [0.1, 0.05, 0.15, 2.0309, 3.5],\n",
    "#         'solver': ['lbfgs']#, 'sag', 'saga'],\n",
    "# #        'penalty' : ['l1', 'l2']\n",
    "#         },\n",
    "\n",
    "# clf_grid_searcher = GridSearchCV(\n",
    "#         estimator=clf,\n",
    "#         param_grid=logit_params,\n",
    "#         scoring='roc_curve',\n",
    "#         n_jobs=-1,\n",
    "#         cv=time_split,\n",
    "#         verbose=10)\n",
    "\n",
    "\n",
    "# clf_grid_searcher.fit(X_train_new, y_train_new)\n",
    "# print(clf_grid_searcher.score(X_train_new, y_train_new))\n",
    "# print(clf_grid_searcher.best_score_, clf_grid_searcher.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# -- RandomForestClassifier :\n",
    "#all : 0.868859388644 {'max_depth': 2, 'max_features': 3}\n",
    "#- day_time - eve -long_sess: 0.866209364039 {'max_depth': 2, 'max_features': 5} \\\n",
    "#               --    -sess_len    0.85821693632 {'max_depth': 10, 'max_features': 5} \\\n",
    "# - year_month  0.881829021147 {'max_depth': 5, 'max_features': 10} / !!!!!!\n",
    "# - week_day    0.863236779933 {'max_depth': 2, 'max_features': 5}  \\\n",
    "# - hour_Val    0.837083132261 {'max_depth': 10, 'max_features': 3}  \\\n",
    "# - hour_cat    0.859024053379 {'max_depth': 5, 'max_features': 3}  \\\n",
    "# - mon_cat     0.871291662775 {'max_depth': 10, 'max_features': 3}   / !!!!!!\n",
    "# - is_morning  0.867803370795 {'max_depth': 10, 'max_features': 5} \\\n",
    "# - is_daytime  0.85062178331 {'max_depth': 5, 'max_features': 3}  \\\n",
    "# - is_evening  0.865505228744 {'max_depth': 10, 'max_features': 5}  \\\n",
    "# - is_long_ses 0.868926633714 {'max_depth': 10, 'max_features': 5}  / !!!!\n",
    "\n",
    "\n",
    "# -- LogisticRegression :\n",
    "#all : 0.840667160623 {'C': 1.0}\n",
    "#- day_time - eve -long_sess:\n",
    "#               --    -sess_len\n",
    "# - year_month     0.856734589393 {'C': 1}  / !!!!!\n",
    "# - week_day       0.838524546676 {'C': 0.15} \\\n",
    "# - hour_Val       0.840135383489 {'C': 1} \\ ~=\n",
    "# - hour_cat       0.78535407395 {'C': 0.15} \\\\\\\n",
    "# + hour_val(ohe): 0.842485256959 {'C': 1} / !!!!\n",
    "# - hour_cat, + hour_val(ohe) 0.840135383489 {'C': 1} \\\n",
    "# - mon_cat        0.832168325842 {'C': 0.05}  \\\n",
    "# - is_morning     0.838739921881 {'C': 1}   \\\n",
    "# - is_daytime     0.84074001553 {'C': 1}  / !!!!\n",
    "# - is_evening     0.840012789159 {'C': 1}  \\\n",
    "# - is_long_ses    0.835454317425 {'C': 1}  \\\n",
    "# all 5quantiles  : 0.840667160623 {'C': 1.0} -\n",
    "# all 2quantiles  : 0.840667160623 {'C': 1} -\n",
    "# all 2quantiles (0.1, 0.9)  : 0.840667160623 {'C': 1} -\n",
    "# +uniq_sites(ohe):0.844042953365 {'C': 1}  / !!!!\n",
    "# +uniq_sites(sc): 0.840681126851 {'C': 1} / !\n",
    "# +uniq_sites + hour_val(ohe) (+hour_cat): 0.84578279885 {'C': 1} / !!!! ???????????\n",
    "# -hour_cat +uniq_sites + hour_val(ohe):  0.843802942673 {'C': 1} / !!!\n",
    "# +hour_cat +uniq_sites - hour_val(ohe):  0.843802942673 {'C': 1} / !!!\n",
    "# -- +year :        0.845713208798 {'C': 1} / !!!!!\n",
    "# -hour +log1p(hour)     0.845895327182 {'C': 1} / !!!!!\n",
    "#   -- -hour_cat         0.786349908391 {'C': 0.15} \\ ---\n",
    "#   -- -year_mon         0.854384041716 {'C': 1} / !!!\n",
    "#0.993002559728\n",
    "#0.909865994457 {'C': 2.0309176209047348, 'random_state': 17, 'solver': 'lbfgs'}\n",
    "# 0.910397293314 {'C': 2.0309, 'solver': 'lbfgs'}\n",
    "\n",
    "\n",
    "\n",
    "# + Tfidf:          0.907521078033 {'C': 3.5}\n",
    "# + Tfidf+ transformer:  0.90336907447 {'C': 3.5}\n",
    "#                        0.903462223502 {'C': 4.6415888336127775}\n",
    "#  -hour +log1p(hour) +year -year_month  0.908940245543 {'C': 3.5} (0.90840662684 {'C': 3.5})\n",
    "#      --      n_iniq :                 -0.85375820649 {'C': 1}\n",
    "\n",
    "\n",
    "# clf = LogisticRegression(\n",
    "#         random_state=17,\n",
    "#         **clf_grid_searcher.best_params_)\n",
    "# clf.fit(X_train_new, y_train_new)\n",
    "# clf.score(X_train_new, y_train_new)\n",
    "# 0.99544488308533252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform_pipeline.steps[0][1].transformer_list[2][1].steps[1][1]\n",
    "#feature_names = [f[0] for f in transform_pipeline.steps[0][1].transformer_list]\n",
    "#feat_importances = pd.Series(clf.feature_importances_, index=feature_names)\n",
    "#feat_importances.nlargest(15).plot(kind='barh')\n",
    "\n",
    "\n",
    "# (0.8884620990228279, {'C': 3.5000000000000013})\n",
    "# weekend+weekday 0.876261489918 {'C': 3.5000000000000013}\n",
    "# weekday  0.876293321626 {'C': 3.5000000000000013}\n",
    "# def : 100000   0.890742942071 {'C': 3.5000000000000013}\n",
    "#                0.890858325471 {'C': 4.200000000000002}\n",
    "#      +seconds+n_unique_sites  0.893673781638 {'C': 3.4000000000000012}\n",
    "\n",
    "\n",
    "# max_train_data = 10000\n",
    "\n",
    "pred_test_generator = KerasBatchClassifier.batch_generator(X_test_nn1, None, BATCH_SIZE, True)\n",
    "add_part = 1 if X_test_nn1.shape[0] % BATCH_SIZE != 0 else 0\n",
    "nn_test_pred = model_nn1.model.predict_generator(\n",
    "    pred_test_generator, \n",
    "    X_test_nn1.shape[0] // BATCH_SIZE + add_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.71030625e-03, 1.86632933e-05, 1.02668775e-04, ...,\n",
       "       4.98598616e-04, 8.60656837e-06, 2.77944288e-04], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_test_pred[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_submission_file(nn_test_pred[:, 1], 'no_ym_fg_nn2.csv')\n",
    "#  ==> 0.87874\n",
    "# logit_test_pred = clf.predict_proba(transformed_test_df)[:, 1]\n",
    "# write_to_submission_file(logit_test_pred, 'no_ym_fg_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==>  0.94952"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
