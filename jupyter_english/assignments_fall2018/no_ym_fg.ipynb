{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import Counter\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function for writing predictions to a file\n",
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)\n",
    "\n",
    "\n",
    "def add_time_features(df, X_sparse):\n",
    "    hour = df['time1'].apply(lambda ts: ts.hour)\n",
    "    morning = ((hour >= 8) & (hour <= 11)).astype('int')\n",
    "    day = ((hour >= 12) & (hour <= 18)).astype('int')\n",
    "    evening = ((hour >= 19) & (hour <= 23)).astype('int')\n",
    "    night = ((hour >= 0) & (hour <= 7)).astype('int')\n",
    "\n",
    "    weekday = df['time1'].apply(lambda ts: ts.weekday())\n",
    "#    weekend = (weekday <= 4).astype('int')\n",
    "\n",
    "\n",
    "    min_d = df[times].min(axis=1)\n",
    "    max_d = df[times].max(axis=1)\n",
    "\n",
    "    # Calculate sessions' duration in seconds\n",
    "    seconds = (max_d - min_d) / np.timedelta64(1, 's')\n",
    "\n",
    "    n_unique_sites = df[df[sites] != 0][sites].apply(\n",
    "            lambda site: site[site != 0].nunique(),\n",
    "            axis=1).astype('float64')\n",
    "\n",
    "    X = hstack([\n",
    "            X_sparse,\n",
    "            morning.values.reshape(-1, 1),\n",
    "            day.values.reshape(-1, 1),\n",
    "            evening.values.reshape(-1, 1),\n",
    "            night.values.reshape(-1, 1),\n",
    "            seconds.values.reshape(-1, 1),\n",
    "            n_unique_sites.values.reshape(-1, 1)\n",
    "#            weekday.values.reshape(-1, 1)\n",
    "#            weekend.values.reshape(-1, 1)\n",
    "            ])\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21669</th>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54843</th>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2013-01-12 09:07:07</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 09:07:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77292</th>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:13</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:14</td>\n",
       "      <td>951.0</td>\n",
       "      <td>2013-01-12 08:50:15</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:15</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>784.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>949.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114021</th>\n",
       "      <td>945</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>949.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>945.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>945.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146670</th>\n",
       "      <td>947</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>950.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>950.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>951.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1               time1  site2               time2  site3  \\\n",
       "session_id                                                                \n",
       "21669          56 2013-01-12 08:05:57   55.0 2013-01-12 08:05:57    NaN   \n",
       "54843          56 2013-01-12 08:37:23   55.0 2013-01-12 08:37:23   56.0   \n",
       "77292         946 2013-01-12 08:50:13  946.0 2013-01-12 08:50:14  951.0   \n",
       "114021        945 2013-01-12 08:50:17  948.0 2013-01-12 08:50:17  949.0   \n",
       "146670        947 2013-01-12 08:50:20  950.0 2013-01-12 08:50:20  948.0   \n",
       "\n",
       "                         time3  site4               time4  site5  \\\n",
       "session_id                                                         \n",
       "21669                      NaT    NaN                 NaT    NaN   \n",
       "54843      2013-01-12 09:07:07   55.0 2013-01-12 09:07:09    NaN   \n",
       "77292      2013-01-12 08:50:15  946.0 2013-01-12 08:50:15  946.0   \n",
       "114021     2013-01-12 08:50:18  948.0 2013-01-12 08:50:18  945.0   \n",
       "146670     2013-01-12 08:50:20  947.0 2013-01-12 08:50:21  950.0   \n",
       "\n",
       "                         time5  ...                 time6  site7  \\\n",
       "session_id                      ...                                \n",
       "21669                      NaT  ...                   NaT    NaN   \n",
       "54843                      NaT  ...                   NaT    NaN   \n",
       "77292      2013-01-12 08:50:16  ...   2013-01-12 08:50:16  948.0   \n",
       "114021     2013-01-12 08:50:18  ...   2013-01-12 08:50:18  947.0   \n",
       "146670     2013-01-12 08:50:21  ...   2013-01-12 08:50:21  946.0   \n",
       "\n",
       "                         time7  site8               time8  site9  \\\n",
       "session_id                                                         \n",
       "21669                      NaT    NaN                 NaT    NaN   \n",
       "54843                      NaT    NaN                 NaT    NaN   \n",
       "77292      2013-01-12 08:50:16  784.0 2013-01-12 08:50:16  949.0   \n",
       "114021     2013-01-12 08:50:19  945.0 2013-01-12 08:50:19  946.0   \n",
       "146670     2013-01-12 08:50:21  951.0 2013-01-12 08:50:22  946.0   \n",
       "\n",
       "                         time9 site10              time10 target  \n",
       "session_id                                                        \n",
       "21669                      NaT    NaN                 NaT      0  \n",
       "54843                      NaT    NaN                 NaT      0  \n",
       "77292      2013-01-12 08:50:17  946.0 2013-01-12 08:50:17      0  \n",
       "114021     2013-01-12 08:50:19  946.0 2013-01-12 08:50:20      0  \n",
       "146670     2013-01-12 08:50:22  947.0 2013-01-12 08:50:22      0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../../data/websites_train_sessions.csv',\n",
    "                       index_col='session_id')\n",
    "test_df = pd.read_csv('../../data/websites_test_sessions.csv',\n",
    "                      index_col='session_id')\n",
    "\n",
    "# Convert time1, ..., time10 columns to datetime type\n",
    "times = ['time%s' % i for i in range(1, 11)]\n",
    "sites = ['site%s' % i for i in range(1, 11)]\n",
    "train_df[times] = train_df[times].apply(pd.to_datetime)\n",
    "test_df[times] = test_df[times].apply(pd.to_datetime)\n",
    "\n",
    "# Sort the data by time\n",
    "train_df = train_df.sort_values(by='time1')\n",
    "\n",
    "# Look at the first rows of the training set\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21669</th>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 08:05:57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54843</th>\n",
       "      <td>56</td>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 08:37:23</td>\n",
       "      <td>56.0</td>\n",
       "      <td>2013-01-12 09:07:07</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2013-01-12 09:07:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77292</th>\n",
       "      <td>946</td>\n",
       "      <td>2013-01-12 08:50:13</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:14</td>\n",
       "      <td>951.0</td>\n",
       "      <td>2013-01-12 08:50:15</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:15</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>784.0</td>\n",
       "      <td>2013-01-12 08:50:16</td>\n",
       "      <td>949.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114021</th>\n",
       "      <td>945</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:17</td>\n",
       "      <td>949.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>945.0</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:18</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>945.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:19</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146670</th>\n",
       "      <td>947</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>950.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>948.0</td>\n",
       "      <td>2013-01-12 08:50:20</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>950.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:21</td>\n",
       "      <td>951.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>946.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>947.0</td>\n",
       "      <td>2013-01-12 08:50:22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1               time1  site2               time2  site3  \\\n",
       "session_id                                                                \n",
       "21669          56 2013-01-12 08:05:57   55.0 2013-01-12 08:05:57    NaN   \n",
       "54843          56 2013-01-12 08:37:23   55.0 2013-01-12 08:37:23   56.0   \n",
       "77292         946 2013-01-12 08:50:13  946.0 2013-01-12 08:50:14  951.0   \n",
       "114021        945 2013-01-12 08:50:17  948.0 2013-01-12 08:50:17  949.0   \n",
       "146670        947 2013-01-12 08:50:20  950.0 2013-01-12 08:50:20  948.0   \n",
       "\n",
       "                         time3  site4               time4  site5  \\\n",
       "session_id                                                         \n",
       "21669                      NaT    NaN                 NaT    NaN   \n",
       "54843      2013-01-12 09:07:07   55.0 2013-01-12 09:07:09    NaN   \n",
       "77292      2013-01-12 08:50:15  946.0 2013-01-12 08:50:15  946.0   \n",
       "114021     2013-01-12 08:50:18  948.0 2013-01-12 08:50:18  945.0   \n",
       "146670     2013-01-12 08:50:20  947.0 2013-01-12 08:50:21  950.0   \n",
       "\n",
       "                         time5  ...                 time6  site7  \\\n",
       "session_id                      ...                                \n",
       "21669                      NaT  ...                   NaT    NaN   \n",
       "54843                      NaT  ...                   NaT    NaN   \n",
       "77292      2013-01-12 08:50:16  ...   2013-01-12 08:50:16  948.0   \n",
       "114021     2013-01-12 08:50:18  ...   2013-01-12 08:50:18  947.0   \n",
       "146670     2013-01-12 08:50:21  ...   2013-01-12 08:50:21  946.0   \n",
       "\n",
       "                         time7  site8               time8  site9  \\\n",
       "session_id                                                         \n",
       "21669                      NaT    NaN                 NaT    NaN   \n",
       "54843                      NaT    NaN                 NaT    NaN   \n",
       "77292      2013-01-12 08:50:16  784.0 2013-01-12 08:50:16  949.0   \n",
       "114021     2013-01-12 08:50:19  945.0 2013-01-12 08:50:19  946.0   \n",
       "146670     2013-01-12 08:50:21  951.0 2013-01-12 08:50:22  946.0   \n",
       "\n",
       "                         time9 site10              time10 target  \n",
       "session_id                                                        \n",
       "21669                      NaT    NaN                 NaT      0  \n",
       "54843                      NaT    NaN                 NaT      0  \n",
       "77292      2013-01-12 08:50:17  946.0 2013-01-12 08:50:17      0  \n",
       "114021     2013-01-12 08:50:19  946.0 2013-01-12 08:50:20      0  \n",
       "146670     2013-01-12 08:50:22  947.0 2013-01-12 08:50:22      0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert time1, ..., time10 columns to datetime type\n",
    "times = ['time%s' % i for i in range(1, 11)]\n",
    "sites = ['site%s' % i for i in range(1, 11)]\n",
    "train_df[times] = train_df[times].apply(pd.to_datetime)\n",
    "test_df[times] = test_df[times].apply(pd.to_datetime)\n",
    "\n",
    "# Sort the data by time\n",
    "train_df = train_df.sort_values(by='time1')\n",
    "\n",
    "# Look at the first rows of the training set\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data/site_dic.pkl', \"rb\") as inp_file:\n",
    "    site_dic = pickle.load(inp_file)\n",
    "\n",
    "inv_site_dic = {v: k for k, v in site_dic.items()}\n",
    "# inv_site_dic.update({0: ''})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[sites] = train_df[sites].fillna(0)\n",
    "test_df[sites] = test_df[sites].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AAA():\n",
    "    X_train = train_df[sites].apply(\n",
    "            lambda x: \" \".join(\n",
    "                    [inv_site_dic[a] for a in x.values if a != 0]), axis=1)\n",
    "\n",
    "    X_train = X_train.apply(lambda x: x.replace('.', ' '))\n",
    "\n",
    "    X_test = test_df[sites].apply(\n",
    "            lambda x: \" \".join(\n",
    "                    [inv_site_dic[a] for a in x.values if a != 0]), axis=1)\n",
    "\n",
    "    X_test = X_test.apply(lambda x: x.replace('.', ' '))\n",
    "\n",
    "    y_train = train_df['target'].astype('int')\n",
    "\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "            [(\"vectorize\",\n",
    "              TfidfVectorizer(ngram_range=(1, 3),\n",
    "                              max_features=100000)),\n",
    "            (\"tfidf\", TfidfTransformer())])\n",
    "\n",
    "    pipeline.fit(X_train.ravel(), y_train)\n",
    "\n",
    "\n",
    "    X_train = pipeline.transform(X_train.ravel())\n",
    "    X_test = pipeline.transform(X_test.ravel())\n",
    "\n",
    "\n",
    "    print(type(X_train))    # scipy.sparse.csr.csr_matrix\n",
    "\n",
    "    print(X_train.shape)   # (253561, 250000)\n",
    "\n",
    "\n",
    "    X_train = add_time_features(train_df, X_train)\n",
    "    X_test = add_time_features(test_df, X_test)\n",
    "\n",
    "\n",
    "    print(X_train.shape, X_test.shape)  #  ((253561, 250004), (82797, 250004))\n",
    "\n",
    "    time_split = TimeSeriesSplit(n_splits=12)\n",
    "    logit = LogisticRegression(C=1, random_state=17)\n",
    "\n",
    "    # c_values = np.logspace(-2,2, 10)\n",
    "    # c_values = np.arange(0,5.,step=0.5)\n",
    "    # c_values = np.concatenate((np.arange(0.1,1,0.1), np.arange(1,5,0.5)))\n",
    "    c_values = np.concatenate((np.arange(0.5, 2,step=0.5), np.arange(2, 3.6, 0.1)))\n",
    "\n",
    "    logit_grid_searcher = GridSearchCV(\n",
    "            estimator=logit,\n",
    "            param_grid={'C': c_values},\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=-1,\n",
    "            cv=time_split,\n",
    "            verbose=10)\n",
    "\n",
    "\n",
    "    logit_grid_searcher.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    print(logit_grid_searcher.best_score_, logit_grid_searcher.best_params_)\n",
    "    # (0.8884620990228279, {'C': 3.5000000000000013})\n",
    "    # weekend+weekday 0.876261489918 {'C': 3.5000000000000013}\n",
    "    # weekday  0.876293321626 {'C': 3.5000000000000013}\n",
    "    # def : 100000   0.890742942071 {'C': 3.5000000000000013}\n",
    "    #                0.890858325471 {'C': 4.200000000000002}\n",
    "    #      +seconds+n_unique_sites  0.893673781638 {'C': 3.4000000000000012}\n",
    "\n",
    "    logit_test_pred = logit_grid_searcher.predict_proba(X_test)[:, 1]\n",
    "    write_to_submission_file(logit_test_pred, 'submit.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special transformer to save output shape\n",
    "class ShapeSaver(BaseEstimator, TransformerMixin):\n",
    "    def transform(self, X):\n",
    "        self.shape = X.shape\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "## Helper functions that extract different data\n",
    "#####################################\n",
    "\n",
    "# Return sites columns as a single string\n",
    "# This string can be supplied into CountVectorizer or TfidfVectorizer\n",
    "\n",
    "def extract_sites_as_string(X):\n",
    "    #return X[sites].astype('str').apply(' '.join, axis=1)\n",
    "    return X['sites_str']\n",
    "\n",
    "\n",
    "\n",
    "# Year-month feature from A4\n",
    "def feature_year_month(X):\n",
    "    return pd.DataFrame(X['time1'].dt.year * 100 + X['time1'].dt.month)\n",
    "\n",
    "def feature_year_month_log1p(X):\n",
    "    return pd.DataFrame(np.log1p(X['time1'].dt.year * 100 + X['time1'].dt.month))\n",
    "\n",
    "# yearfeature from A4\n",
    "def feature_year(X):\n",
    "    return pd.DataFrame(X['time1'].dt.year)\n",
    "\n",
    "# Hour feature from A4\n",
    "def feature_hour(X):\n",
    "    return pd.DataFrame(X['time1'].dt.hour)\n",
    "\n",
    "# Hour feature from A4\n",
    "def feature_hour_log(X):\n",
    "    return np.log1p(pd.DataFrame(X['time1'].dt.hour))\n",
    "\n",
    "\n",
    "# Month\n",
    "def feature_month(X):\n",
    "    return pd.DataFrame(X['time1'].dt.month)\n",
    "\n",
    "# Weekday\n",
    "def feature_weekday(X):\n",
    "    return pd.DataFrame(X['time1'].dt.weekday)\n",
    "\n",
    "# Is day feature from A4\n",
    "def feature_is_daytime(X):\n",
    "    return pd.DataFrame( (X['time1'].dt.hour >= 12) & (X['time1'].dt.hour <= 18))\n",
    "\n",
    "# Is evening feature from A4\n",
    "def feature_is_evening(X):\n",
    "    return pd.DataFrame( (X['time1'].dt.hour >= 19) & (X['time1'].dt.hour <= 23))\n",
    "\n",
    "# Is morning feature from A4\n",
    "def feature_is_morning(X):\n",
    "    return pd.DataFrame(X['time1'].dt.hour <= 11)\n",
    "\n",
    "# Long Session length feature from A4\n",
    "def feature_is_long_session(X):\n",
    "    X['session_end_time'] = X[times].max(axis=1)\n",
    "    session_duration = (X['session_end_time'] - X['time1']).astype('timedelta64[s]')\n",
    "#    q = session_duration.quantile([0.1, 0.90]).values\n",
    "    X['long_session_duration'] = 0\n",
    "    X[session_duration < 10]['long_session_duration'] = 1\n",
    "    X[session_duration < 20]['long_session_duration'] = 2\n",
    "    X[session_duration < 100]['long_session_duration'] = 3\n",
    "    X[session_duration < 500]['long_session_duration'] = 4\n",
    "    X[session_duration < 1000]['long_session_duration'] = 5\n",
    "#    X[(session_duration > q[1]) & (session_duration <= q[2])]['long_session_duration'] = 2\n",
    "#    X[(session_duration > q[2]) & (session_duration <= q[3])]['long_session_duration'] = 3\n",
    "#    X[(session_duration > q[3]) & (session_duration <= q[4])]['long_session_duration'] = 4\n",
    "#    X[session_duration > q[1]]['long_session_duration'] = 2\n",
    "    return X[['long_session_duration']]\n",
    "\n",
    "# Session length feature from A4\n",
    "def feature_session_len(X):\n",
    "    X['session_end_time'] = X[times].max(axis=1)\n",
    "    X['session_duration'] = (X['session_end_time'] - X['time1']).astype('timedelta64[s]')\n",
    "    return X[['session_duration']]\n",
    "\n",
    "# uniq sites per session\n",
    "def feature_uniq_sites(X):\n",
    "    X['n_unique_sites'] = X[X[sites] != 0][sites].apply(\n",
    "            lambda site: site[site != 0].nunique(), axis=1).astype('float64')\n",
    "\n",
    "    return X[['n_unique_sites']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_pipeline = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        # List of features goes here:\n",
    "#        ('year_month_val', Pipeline([\n",
    "#            ('extract', FunctionTransformer(feature_year_month, validate=False)),\n",
    "#            ('scale', StandardScaler()),\n",
    "#            ('shape', ShapeSaver())\n",
    "#        ])),\n",
    "        ('session_len', Pipeline([\n",
    "            ('extract', FunctionTransformer(feature_session_len, validate=False)),\n",
    "            ('scale', StandardScaler()),\n",
    "            ('shape', ShapeSaver())\n",
    "        ])),\n",
    "        ('weekday_cat', Pipeline([\n",
    "            ('extract', FunctionTransformer(feature_weekday, validate=False)),\n",
    "            ('ohe', OneHotEncoder()),\n",
    "            ('shape', ShapeSaver())\n",
    "        ])),\n",
    "#        ('hour_val', Pipeline([\n",
    "#            ('extract', FunctionTransformer(feature_hour, validate=False)),\n",
    "##            ('scale', StandardScaler()),\n",
    "#            ('ohe', OneHotEncoder()),\n",
    "#            ('shape', ShapeSaver())\n",
    "#         ])),\n",
    "        ('hour_val_log1p', Pipeline([\n",
    "            ('extract', FunctionTransformer(feature_hour_log, validate=False)),\n",
    "            ('scale', StandardScaler()),\n",
    "            ('shape', ShapeSaver())\n",
    "         ])),\n",
    "        ('hour_cat', Pipeline([\n",
    "            ('extract', FunctionTransformer(feature_hour, validate=False)),\n",
    "            ('ohe', OneHotEncoder()),\n",
    "            ('shape', ShapeSaver())\n",
    "         ])),\n",
    "        ('month_cat', Pipeline([\n",
    "            ('extract', FunctionTransformer(feature_month, validate=False)),\n",
    "            ('ohe', OneHotEncoder()),\n",
    "            ('shape', ShapeSaver())\n",
    "         ])),\n",
    "        ('is_morning', Pipeline([\n",
    "            ('extract', FunctionTransformer(feature_is_morning, validate=False)),\n",
    "            ('shape', ShapeSaver())\n",
    "         ])),\n",
    "        ('is_daytime', Pipeline([\n",
    "            ('extract', FunctionTransformer(feature_is_daytime, validate=False)),\n",
    "            ('shape', ShapeSaver())\n",
    "         ])),\n",
    "        ('is_evening', Pipeline([\n",
    "            ('extract', FunctionTransformer(feature_is_evening, validate=False)),\n",
    "            ('shape', ShapeSaver())\n",
    "         ])),\n",
    "        ('is_long_session', Pipeline([\n",
    "            ('extract', FunctionTransformer(feature_is_long_session, validate=False)),\n",
    "            ('ohe', OneHotEncoder()),\n",
    "            ('shape', ShapeSaver())\n",
    "         ])),\n",
    "#        ('feature_uniq_sites', Pipeline([\n",
    "#            ('extract', FunctionTransformer(feature_uniq_sites, validate=False)),\n",
    "#            ('ohe', OneHotEncoder()),\n",
    "#            ('shape', ShapeSaver())\n",
    "#         ])),\n",
    "        ('year', Pipeline([\n",
    "            ('extract', FunctionTransformer(feature_year, validate=False)),\n",
    "            ('ohe', OneHotEncoder()),\n",
    "            ('shape', ShapeSaver())\n",
    "         ])),\n",
    "        ('sites_tfidf', Pipeline([\n",
    "            ('extract', FunctionTransformer(extract_sites_as_string, validate=False)),\n",
    "            ('count', TfidfVectorizer(token_pattern=r'(?u)\\b\\w+\\b',\n",
    "                                      ngram_range=(1, 3),\n",
    "                                      max_features=100000)),\n",
    "            (\"tfidf\", TfidfTransformer()),\n",
    "            ('shape', ShapeSaver())\n",
    "        ])),\n",
    "        # Add more features here :)\n",
    "        # ...\n",
    "    ]))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253561, 100044) (82797, 100044)\n"
     ]
    }
   ],
   "source": [
    "# Run preprocessing on full data\n",
    "x_train_new = train_df.iloc[:, :-1]\n",
    "x_train_new['sites_str'] = train_df[sites].apply(\n",
    "        lambda x: \" \".join(\n",
    "                [inv_site_dic[a] for a in x.values if a != 0]), axis=1)\n",
    "\n",
    "x_train_new['sites_str'] = x_train_new['sites_str'].apply(lambda x: x.replace('.', ' '))\n",
    "\n",
    "x_test_new = test_df.iloc[:, :]\n",
    "x_test_new['sites_str'] = test_df[sites].apply(\n",
    "        lambda x: \" \".join(\n",
    "                [inv_site_dic[a] for a in x.values if a != 0]), axis=1)\n",
    "\n",
    "x_test_new['sites_str'] = x_test_new['sites_str'].apply(lambda x: x.replace('.', ' '))\n",
    "\n",
    "transformed_train_df = transform_pipeline.fit_transform(x_train_new)\n",
    "transformed_test_df = transform_pipeline.transform(x_test_new)\n",
    "\n",
    "X_train_new = transformed_train_df\n",
    "y_train_new = train_df['target']\n",
    "\n",
    "print(transformed_train_df.shape, transformed_test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12 folds for each of 5 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   18.0s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   25.8s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   51.0s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  1.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  1.8min finished\n",
      "D:\\DiskD\\Eldaniz\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:757: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9931947218095609\n",
      "0.9104206626981047 {'C': 2.0309, 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "time_split = TimeSeriesSplit(n_splits=12)\n",
    "\n",
    "logit = LogisticRegression(C=1, random_state=17)\n",
    "\n",
    "# c_values = np.logspace(-2,2, 10)\n",
    "# c_values = np.arange(0,5.,step=0.5)\n",
    "# c_values = np.concatenate((np.arange(0.1,1,0.1), np.arange(1,5,0.5)))\n",
    "c_values = np.concatenate((np.arange(0.5, 2,step=0.5), np.arange(2, 3.6, 0.1)))\n",
    "\n",
    "\n",
    "clf = LogisticRegression(C=1, random_state=17)  # RandomForestClassifier(random_state=17)\n",
    "\n",
    "tree_params = {\n",
    "        'max_depth': [2, 5, 10],\n",
    "        'max_features': [3, 5, 20]}\n",
    "\n",
    "#c_values = np.logspace(-4, 10, 40)\n",
    "logit_params = {\n",
    "        'C': [0.1, 0.05, 0.15, 2.0309, 3.5],\n",
    "        'solver': ['lbfgs']#, 'sag', 'saga'],\n",
    "#        'penalty' : ['l1', 'l2']\n",
    "        },\n",
    "\n",
    "clf_grid_searcher = GridSearchCV(\n",
    "        estimator=clf,\n",
    "        param_grid=logit_params,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        cv=time_split,\n",
    "        verbose=10)\n",
    "\n",
    "\n",
    "clf_grid_searcher.fit(X_train_new, y_train_new)\n",
    "print(clf_grid_searcher.score(X_train_new, y_train_new))\n",
    "print(clf_grid_searcher.best_score_, clf_grid_searcher.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# -- RandomForestClassifier :\n",
    "#all : 0.868859388644 {'max_depth': 2, 'max_features': 3}\n",
    "#- day_time - eve -long_sess: 0.866209364039 {'max_depth': 2, 'max_features': 5} \\\n",
    "#               --    -sess_len    0.85821693632 {'max_depth': 10, 'max_features': 5} \\\n",
    "# - year_month  0.881829021147 {'max_depth': 5, 'max_features': 10} / !!!!!!\n",
    "# - week_day    0.863236779933 {'max_depth': 2, 'max_features': 5}  \\\n",
    "# - hour_Val    0.837083132261 {'max_depth': 10, 'max_features': 3}  \\\n",
    "# - hour_cat    0.859024053379 {'max_depth': 5, 'max_features': 3}  \\\n",
    "# - mon_cat     0.871291662775 {'max_depth': 10, 'max_features': 3}   / !!!!!!\n",
    "# - is_morning  0.867803370795 {'max_depth': 10, 'max_features': 5} \\\n",
    "# - is_daytime  0.85062178331 {'max_depth': 5, 'max_features': 3}  \\\n",
    "# - is_evening  0.865505228744 {'max_depth': 10, 'max_features': 5}  \\\n",
    "# - is_long_ses 0.868926633714 {'max_depth': 10, 'max_features': 5}  / !!!!\n",
    "\n",
    "\n",
    "# -- LogisticRegression :\n",
    "#all : 0.840667160623 {'C': 1.0}\n",
    "#- day_time - eve -long_sess:\n",
    "#               --    -sess_len\n",
    "# - year_month     0.856734589393 {'C': 1}  / !!!!!\n",
    "# - week_day       0.838524546676 {'C': 0.15} \\\n",
    "# - hour_Val       0.840135383489 {'C': 1} \\ ~=\n",
    "# - hour_cat       0.78535407395 {'C': 0.15} \\\\\\\n",
    "# + hour_val(ohe): 0.842485256959 {'C': 1} / !!!!\n",
    "# - hour_cat, + hour_val(ohe) 0.840135383489 {'C': 1} \\\n",
    "# - mon_cat        0.832168325842 {'C': 0.05}  \\\n",
    "# - is_morning     0.838739921881 {'C': 1}   \\\n",
    "# - is_daytime     0.84074001553 {'C': 1}  / !!!!\n",
    "# - is_evening     0.840012789159 {'C': 1}  \\\n",
    "# - is_long_ses    0.835454317425 {'C': 1}  \\\n",
    "# all 5quantiles  : 0.840667160623 {'C': 1.0} -\n",
    "# all 2quantiles  : 0.840667160623 {'C': 1} -\n",
    "# all 2quantiles (0.1, 0.9)  : 0.840667160623 {'C': 1} -\n",
    "# +uniq_sites(ohe):0.844042953365 {'C': 1}  / !!!!\n",
    "# +uniq_sites(sc): 0.840681126851 {'C': 1} / !\n",
    "# +uniq_sites + hour_val(ohe) (+hour_cat): 0.84578279885 {'C': 1} / !!!! ???????????\n",
    "# -hour_cat +uniq_sites + hour_val(ohe):  0.843802942673 {'C': 1} / !!!\n",
    "# +hour_cat +uniq_sites - hour_val(ohe):  0.843802942673 {'C': 1} / !!!\n",
    "# -- +year :        0.845713208798 {'C': 1} / !!!!!\n",
    "# -hour +log1p(hour)     0.845895327182 {'C': 1} / !!!!!\n",
    "#   -- -hour_cat         0.786349908391 {'C': 0.15} \\ ---\n",
    "#   -- -year_mon         0.854384041716 {'C': 1} / !!!\n",
    "#0.993002559728\n",
    "#0.909865994457 {'C': 2.0309176209047348, 'random_state': 17, 'solver': 'lbfgs'}\n",
    "# 0.910397293314 {'C': 2.0309, 'solver': 'lbfgs'}\n",
    "\n",
    "\n",
    "\n",
    "# + Tfidf:          0.907521078033 {'C': 3.5}\n",
    "# + Tfidf+ transformer:  0.90336907447 {'C': 3.5}\n",
    "#                        0.903462223502 {'C': 4.6415888336127775}\n",
    "#  -hour +log1p(hour) +year -year_month  0.908940245543 {'C': 3.5} (0.90840662684 {'C': 3.5})\n",
    "#      --      n_iniq :                 -0.85375820649 {'C': 1}\n",
    "\n",
    "\n",
    "clf = LogisticRegression(\n",
    "        random_state=17,\n",
    "        **clf_grid_searcher.best_params_)\n",
    "clf.fit(X_train_new, y_train_new)\n",
    "clf.score(X_train_new, y_train_new)\n",
    "# 0.99544488308533252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform_pipeline.steps[0][1].transformer_list[2][1].steps[1][1]\n",
    "#feature_names = [f[0] for f in transform_pipeline.steps[0][1].transformer_list]\n",
    "#feat_importances = pd.Series(clf.feature_importances_, index=feature_names)\n",
    "#feat_importances.nlargest(15).plot(kind='barh')\n",
    "\n",
    "\n",
    "# (0.8884620990228279, {'C': 3.5000000000000013})\n",
    "# weekend+weekday 0.876261489918 {'C': 3.5000000000000013}\n",
    "# weekday  0.876293321626 {'C': 3.5000000000000013}\n",
    "# def : 100000   0.890742942071 {'C': 3.5000000000000013}\n",
    "#                0.890858325471 {'C': 4.200000000000002}\n",
    "#      +seconds+n_unique_sites  0.893673781638 {'C': 3.4000000000000012}\n",
    "\n",
    "logit_test_pred = clf.predict_proba(transformed_test_df)[:, 1]\n",
    "write_to_submission_file(logit_test_pred, 'no_ym_fg_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==>  0.94952"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
